{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Cifar-10 Dataset Using CNN on Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I am going to make a classifier for CIFAR-10 Dataset using Convolutional Neural Network. The classifier is CNN based model. The CNN model should be feed with the images that has been preprocessed and normalized first. The classifier should then test the test set and make a classification of each test set. This notebook was run in NVIDIA GTX 1050, and intel Core i5 with cuda 10.1, cudnn 10.1, using tensorflow 1.13.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data \n",
    "The dataset of Cifar-10 can be found here : https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "For a simplicity purpose, I have reupload the Cifar-10 dataset with .csv format in my github. The label is in the last column. In this notebook I will be using this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the preprocessing library that will be useful for our project. Pandas package is use for data structure and data manipulation. preprocessing package from sklearn is used to normalize our images. train_test_split package is used to split our dataset as test images, train images, and validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset (images and labels) then store it in dataset variable. Our images size is 32x32x3 = 3072 columns. 32x32 is the image size and 3 means RGB. The last column is our labels. We store the images in X_data variable and the labels in y_data variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "X_data = dataset.iloc[:, :-1].values\n",
    "y_data = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels is an integer from 0 - 9 which is stand for : <br>\n",
    "- airplane   : 0 <br>\n",
    "- automobile : 1 <br>\n",
    "- bird       : 2 <br>\n",
    "- cat        : 3 <br>\n",
    "- deer       : 4 <br>\n",
    "- dog        : 5 <br>\n",
    "- frog       : 6 <br>\n",
    "- horse      : 7 <br>\n",
    "- ship       : 8 <br>\n",
    "- truck      : 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Check out the random picture that is stored in our dataset and match it with the label. To do that, we should reshape our sample image from the shape of (_,3072) into (_,32,32,3). Then we visualize our data using matplotlib package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our shape of random image :  (3072,)\n",
      "Our new shape of images :  (32, 32, 3)\n",
      "\n",
      "Our label is :  1\n",
      "Our image is :\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHipJREFUeJztnWmMnNd1pt9Ta69cms2luZiLzFEiL5KVhqJYtiJZTqAYHsgajA37h6EfRhgEMRAjmR+CMhh7gvxwMmM7RjLjgI6UKIHjJbEFMzOeGWuEDITEHlnURlGibFES9yabSzd7r+0786OKA6p139vFXqop3/cBCFbfU/e7t+5Xp76q+37nHHN3CCHSI7faExBCrA5yfiESRc4vRKLI+YVIFDm/EIki5xciUeT8QiSKnF+IRJHzC5EohaV0NrN7AXwVQB7AX7r7F2PPX7N2nW/cNESs/E5Ds/BnVC5ntI9HPtdi9zQa+DGNdOQ9FhjNYvNf1BFh9I7NyFiRA0bv/4y/8GsfbAVY7tHi01/caKxXfKiw9eLoGUxOjLV1Zhbt/GaWB/BfAPwagFMAnjazA+7+MuuzcdMQvvinjwRtWZbRsbrL5WB7qauL9sny4T4AUHf+wVBAntryjXB7kU89+m7xAp9HjX3SIP6myDWI1Yu0T73Gj9jIkRcNLMr5Y7eTR281j4yVZZH5k47RD9fIPGLv00Yjslax8Uh7PbpW4Xn84e99ou1xl/K1/zYAR939dXevAvgWgPuWcDwhRAdZivNvA3Dyqr9PtdqEEG8DluL8oe9Tb/meYmb7zOygmR2cuDy2hOGEEMvJUpz/FIAdV/29HcCZ+U9y9/3uPuzuw2vWrl/CcEKI5WQpzv80gL1mttvMSgA+CeDA8kxLCLHSLHq3393rZvZZAP8LTanvEXd/aaF+Gdm1LZT5bnQ1C++iTl+epH2KvXx7OF/spjY475eRneN6ZGe+MVejtrnLs9RW6uJqRQN8x3lqdirYnjN+vL7etdTmkbGyyO62ERlzsbvskSWO7vazcxYTFmI7+rE5xnb72XoAQEZWJVuk6tAuS9L53f0HAH6w5FkIITqO7vATIlHk/EIkipxfiESR8wuRKHJ+IRJlSbv910oja2BiOixF1WpcErtw/mKw/dTpUdon39VLbX39/Gajco5LYkwFrNb53LNandpmJsNrAQDdRT4P5LjMM1kNy5/VKpea9uzeS23vvGEntXXHAquIFBWVqCLBOx4xZjEdkMU5LTbAaJHEpL4ceW1ZRGZdDnTlFyJR5PxCJIqcX4hEkfMLkShyfiESpaO7/VPT0/jR//0xsfGd7xzCQT+zFb4rO9cIKwQAUCxxWz7jn4cNsmE753xHvxHZie4t8d3ybuOnpqvMU401ctVg+/Q0VyQOHnqO2kYvvCVK+/+zZ/duahscHAy2d/f00D4eS8cVCZrJSEorADB2PjudSzAWLMSCoBYR2HMtSoWu/EIkipxfiESR8wuRKHJ+IRJFzi9Eosj5hUiUzgb2NDKMT4Xz1nkkd56R6IxCief964lIZfkct5VQorY5hOWmeuQzdHJmmtpmp7mtbFzO63Me9JMnL61Y5nkL56bmqO21k6ep7fjIWWpbtyacF3DH9u20z8bBDfx463kwViEXqbJEZMDFBu+wgkgAzxe40His+k48h9/SpUpd+YVIFDm/EIki5xciUeT8QiSKnF+IRJHzC5EoS5L6zOwYgEkADQB1dx+OPT9zx2w1LGsUi7GpkKinBo9Uc3Cb5SNllSIKSrUWlsRqkan39/RR2+TEDLVNVHkpr0okQqxUCkuV/SX+wvJ5Lm9O1yu8XyQCsnLhcrB9fJxHb/b2cTlyaGgrtd2wew+19ZXCsmiZrBMQzydZi6TVc3DJMRZ5yGTAmBrJJMdYrsP5LIfOf7e7X1iG4wghOoi+9guRKEt1fgfwQzN7xsz2LceEhBCdYalf++9w9zNmtgnA42b2irs/efUTWh8K+wCgq3fNEocTQiwXS7ryu/uZ1v+jAB4DcFvgOfvdfdjdh0tdfENHCNFZFu38ZtZrZv1XHgP4dQCHl2tiQoiVZSlf+zcDeKxVhqgA4O/c/X/GOmTumK2E5bJKjX8OsVJHXZFyUbGYp0gAYbT0E7NNR5KPdnXzwcrFSCLOGu83V+EyYN1IFFvkdZUiUXHxywM/ZqEQPmZsHpMzfB0vv3qE2i5c5GJTf1c4unD7Nh5duD4SQViKREfG6o1ldZ7ktU5UwFi0aMPDcnVHpD53fx3AzYvtL4RYXST1CZEocn4hEkXOL0SiyPmFSBQ5vxCJ0tEEnu6OKolusgaPemJ1ybJc+7LGmyhHEi3m+edhlgvLNYXIKtYi0XmlApcq+7p51NlMlSfcrCM8x0hZQ1Tq3FiOJDvNR6LYnFxXallE8iIJUgEgl+Pn5eylUWo7UwnXZTx6/ATts3FjuM4gAGzduoPa+vr6qa2rHJGlidRa84jUR2oXNq4hsaeu/EIkipxfiESR8wuRKHJ+IRJFzi9EonR2tx9APZLLjNEgO8RzU5O0TyGyBd+IiASFXJXaWEBQscgPWIgtcSQXXyyZYF+kTFmdfJxH0u2hFplHvcHXI2f8oE6iVRqRHf1GPpa0jptiue7MwmtVjyTjmzgzRm3HR45RW7nEd/R7enqojQWoxfIMFovh11Wt8LyQ89GVX4hEkfMLkShyfiESRc4vRKLI+YVIFDm/EInS8cCeSi0sHbE8fQCQkWAFVuYIAOqRPHezETmkGJHR8kTaKhd4Hyc59QDAPFLeKSK/ecZ1LxbXMdPgATVV8LFykfx+1cg5KxJd1HN8rFqOv66YnJfLR3IQWjgIKhInFM3/mEU00+osz0E4MR3RKpmcWuHHY/4yOzPBx5mHrvxCJIqcX4hEkfMLkShyfiESRc4vRKLI+YVIlAWlPjN7BMBHAYy6+7tbbQMAvg1gF4BjAD7h7jwUqkWWZZiZC0svhZj2kpFpRuSw2elz1FYqcTFnYDMv49RN1JpcREbLR3Lxea5GbZfHwrnnAGB2iss5O3ffGGyfrPXSPmNjl6mtXObRaDUi2wKAkTC8LKbZ8WWM9mtEDllCeI1z+UguwUiptEYsPDIW5ViZprZs/GSw/eLp1/lYJL9fLSI3zqedK/9fA7h3XtuDAJ5w970Anmj9LYR4G7Gg87v7kwAuzWu+D8CjrcePAvjYMs9LCLHCLPY3/2Z3HwGA1v+blm9KQohOsOK395rZPgD7AKDQxX93CiE6y2Kv/OfMbAgAWv/Tqgnuvt/dh919OF8qL3I4IcRys1jnPwDggdbjBwB8f3mmI4ToFO1Ifd8EcBeAQTM7BeDzAL4I4Dtm9hkAJwB8vJ3BHI5GnUgsEblmfbk72L6ml8tQsz2Rl2ZcoipO8WjALpIdc9MmvuUx182TOlbrXOrr7uKvLd8TXg8A6FmzJti+rneI9tkyWKG2WHThXER+myH9zp7nEmxtepzais7XqlDn5cvyWfhc12qR5K95vvYZ+PnMIqXNMMvHmzhzLNheGeNrNTUVPmd1kjg1xILO7+6fIqZ72h5FCHHdoTv8hEgUOb8QiSLnFyJR5PxCJIqcX4hE6WgCT7gD9bD0srann3ZbR2S70yMnaJ/ZyA1FlUgUnp09Tm27N4QlvU07ttE+r5w5Q22e8eixnmkuOa7t5XLTiydfCLb3beFRZX1lnoD0jZ+9TG2N3vXUtm7ve8NjbX0n7TN9/Ai15SORjGucR7LNTIXlw5lJel8aSsU+apuY48lCu9dtpLYN3fxcT5HIQ0RqShqLgo0kjJ2PrvxCJIqcX4hEkfMLkShyfiESRc4vRKLI+YVIlI5LfblGWNbY0sfllXNjYVmm1s+1kEI/lw5zxuWaeo3nId1567uC7WORWnfV9ZHoPOPLn1vD5bzxCR4hNjkXlgizGR4xV5nj0ufayDxOTnGJbfp8OAHpznXraJ+tN4blQQAYf5lH7k2f5vLs2LmwbWKaJ0htkOhNALg8y99z3eu51Ne/g9vqpL7e3CyPtmQ1FC2mD84/RtvPFEL8XCHnFyJR5PxCJIqcX4hEkfMLkSgd3e0v5PMYWBPehR/s47vz45fCucwGunhASrnIdz3rNb67vemGcLkrANgztCPY/tIJXlZpXZmX66pHyl1t2sJ3xXODXBmZLoQ/z3P9fB5j589S285NvHzZTInPf6wRDiS6NHae9skNvYPatt90O7WdPvUKtc3NzgTbi3n+/vBI/a98xnMJVsZ5sNB5cIWmPhOeYy7Pr80NUjruWtCVX4hEkfMLkShyfiESRc4vRKLI+YVIFDm/EInSTrmuRwB8FMCou7+71fYFAL8J4Ipu85C7/2ChY5WKeezcMhC0/Zvf+BDtd/z1XcH2yTkeWFKZ4zJUvcKlvl1budzkWVgC8sEttM/liJw3PcPnv32QlwCrOw8kmpoOB8B4F89p2Oc8F18+45rS5rW8bNj0aFjSmzodlrUAoFbhr6t3M5cct77rg9SW1S4H20fPvEb7zExxWQ6R9VjTywPGCuA5GZ14YW2Gj+UkgMcjJdTm086V/68B3Bto/4q739L6t6DjCyGuLxZ0fnd/EsClDsxFCNFBlvKb/7NmdsjMHjEz/r1RCHFdsljn/xqAGwDcAmAEwJfYE81sn5kdNLODFZJoQgjReRbl/O5+zt0b7p4B+DqA2yLP3e/uw+4+XO7iG0RCiM6yKOc3s6Gr/rwfwOHlmY4QolO0I/V9E8BdAAbN7BSAzwO4y8xuAeAAjgH4rXYGy5tjTT4sRf3KrVxiu+1d4XJYkzM8x1nN+edarc7lkPoM/2kyOxceb3eVl+uaqXC5ZipSkqtY5KdmbIKXruraHY7em63wtfJ1g9R2+uwItb36Bi+XdtP6sFR54nxk7zjjUlmji0d99u28ldo+eMOuYPulk1zq++mzz1Db6NmfUluv8fyPqPByaXMNko8v49JnoRjuUyU5MoPHWOgJ7v6pQPPDbY8ghLgu0R1+QiSKnF+IRJHzC5Eocn4hEkXOL0SidDSBZ1avY+pSWA459Qa/VWD7tt3B9m1Dm2mfQg+XhrJImayJCxeobXw8PPcNAxton+lZLr3MzEYi/qa4NDQ5tZbabrxhT/h40xGpaZZLjhu7eTRgscJf2y/98vuD7ZdmeJ9jZ8MReABQzfGyYY1ZXsoLpITW1veG31MAsPG9v0Zt9bFwMlkAuHTkKWp74/DT1HbhtZ8F23Mlfs5yhbAMaJHktG85RtvPFEL8XCHnFyJR5PxCJIqcX4hEkfMLkShyfiESpaNSXz6Xx7ru3qBt8iKvFzdCopsGt/B6a2vz/KX19vM6eFjLJcK8hWWq/kiagrWRGoSeW1wdvyMv89p0GzeGpa2eHh41ORORFW/exSMWf3WYR9PNksjJmYgStXcHj4A8d5HLkWfO8kjBs2+cDLafiNTjm4vIxN3reCLRde8OpbpscsuNv0Jt2944FGw/9COeGvP82TeC7W48Qep8dOUXIlHk/EIkipxfiESR8wuRKHJ+IRKlo7v9xXweQwPhoBSr8oCPS+dGg+0vHDpK+zx3mOda27xtB7V98FfvpLZtG8NznxvjO6z5QkQKiOz2Fwr81LxjKy+T0N1VDLaXS/xzfk2ph9rQz+dYa/B5TJKAptkGV2iOvHqM2sYq4fJfAHDrnrDCAQBTm8Lr+MYIV5eOHOdqyguv8/fcZJmrSINr+BrftDmsqAzfyQOMnvvx48H240e5cjMfXfmFSBQ5vxCJIucXIlHk/EIkipxfiESR8wuRKObOAxwAwMx2APgbAFsAZAD2u/tXzWwAwLcB7EKzZNcn3D1SrwhY39/ndw2/J2h7zzvC5Z0AYO2GsJTzzEtcknklIhvdcfc91FYHX49/fc8Hgu3ru3ifrm4eJFIocvlndo7Lhxs38LXqKYcDp6qRcl0xLB8pexa5dlgxnHPv1eOnaJ8/+U9fobYLozx455dvD58XAPjoxz8dbPcKz/t3+OmfUNuZOpcqXxrn5bWyPM+F6LPjwfa9EZ84/eqzwfYfPXEAly9d4JO8inau/HUAv+/uvwjgdgC/Y2Y3AXgQwBPuvhfAE62/hRBvExZ0fncfcfdnW48nARwBsA3AfQAebT3tUQAfW6lJCiGWn2v6zW9muwC8D8BTADa7+wjQ/IAAwL+jCCGuO9p2fjPrA/BdAJ9zd14j+q399pnZQTM7WKm1Xz5YCLGytOX8ZlZE0/G/4e7fazWfM7Ohln0IQPAGfHff7+7D7j5cLobvOxdCdJ4Fnd/MDMDDAI64+5evMh0A8EDr8QMAvr/80xNCrBTtRPXdAeDTAF40s+dbbQ8B+CKA75jZZwCcAPDxhQ5Ua2Q4Px6WsF4p8qit/OjFYPuJkRHa58577qK2h/79H1Dbn/35f6W2//6PB4Ltv7CNl+sqlvLU1tu/htoaDZ7PbmDtALVtHAiXMItFCZZKPHIvFyltNtXgCfmqhfB15Wt/8Ve0z8uvvEht5SKf42MH/p7att9IpOW9/4r26S7z0mBrnL/mrX3UhDpZDwCYJpGOXuXy7M5t4ZyMByPrNJ8Fnd/d/xkA0w25YC6EuK7RHX5CJIqcX4hEkfMLkShyfiESRc4vRKJ0NIFnqVzGtl3vDNoamKT9arVwBFapl2srQzt4mSk3HoW3Yysvx/S/v//dYPvkWZ7IsqebR3OVuyPJPanAApQL/Gapvp7wmvR08wjCUkQe6irxOXoXf23nZ8Pn86UjL9M+H/4wF49uvuVmavv6X3L58MdP/o9g+54tPNlmqYfLsxfO8sSfL7z6M2or9vJ13LwmPJfGLJd7u0lC1rbC+Vroyi9Eosj5hUgUOb8QiSLnFyJR5PxCJIqcX4hE6ajU53DUEZYvGhmX30rlsEzVy4PiMDHFE2CeG+URhBcu8Rykp86Gowu9zpOUdJW5xFOrcSknlla1XOSnrbcclgHzBS5fdXfxKLauLi4RZnkuLJ04fy5scN7nY/ffT23vf//7qe3kSZ4U9LED/xhsf+6FnbRPY65KbWPnLlNb9eJpais0eCLXmfpUsP31sZO0T085LM9WKrO0z3x05RciUeT8QiSKnF+IRJHzC5Eocn4hEqWju/31egMXxsM75rU6L59UyIU/o7zOd8ufO3SY2t5z8y9F+vE8cqw8VbXAd/SrNb7LPjJygdrmIuWkSpF8fEUyXCzgo1jigULFiLLQcF6eamouvOs8MBjOMQgAgxt4LsTJCZ4tfsvQFmq7NBZWdn74wx/QPnNT09R28WJ4Zx4Apo1fSwuRAK88UUDWbw6XqQOATZvDr7keyf04H135hUgUOb8QiSLnFyJR5PxCJIqcX4hEkfMLkSgLSn1mtgPA3wDYAiADsN/dv2pmXwDwmwCuaCkPuTvXT9DMndewsDxkeZ5HbmomHKQzO8Vll7Pnw5IiAPzpn/05tR0/epzPoxqWUY6e5oFCHglYipXkqjW4jGYNXsYpTz7PLSL2WSRXnBsvTxXNF+fh193dy+d+8SI/Z+VISbGJy1wGrFTC8z92jAcDWURCrvHTAo8EQcUCtVgOxd4yz1E5Mx2eYxZ5v82nHZ2/DuD33f1ZM+sH8IyZPd6yfcXd/3PbowkhrhvaqdU3AmCk9XjSzI4A4KlxhRBvC67pN7+Z7QLwPgBPtZo+a2aHzOwRM+P5q4UQ1x1tO7+Z9QH4LoDPufsEgK8BuAHALWh+M/gS6bfPzA6a2cF6lSe9EEJ0lrac38yKaDr+N9z9ewDg7ufcveHuGYCvA7gt1Nfd97v7sLsPFyL3kAshOsuCzm9mBuBhAEfc/ctXtQ9d9bT7AfBIGiHEdUc7u/13APg0gBfN7PlW20MAPmVmt6CpYhwD8FsLDlYoYGDDALHy6LdZEmVViZTrykUirMbHxqltw8ZN1LZ2IBxlVY/IK5nzfHD1Gpe9GnUuscVy/2W18FxismKlwueYEckOABCJ6suR68p4JDrvX370L9R29913U9tLLx+hNvayq5Fzlo+8F7PI+yomzzYqkZ+81fBcTh7nOfzy5XBOwNo1/LRuZ7f/nxGWdKOavhDi+kZ3+AmRKHJ+IRJFzi9Eosj5hUgUOb8QiWIek3KWmbUDa/0D93wgaMsi0VKkwhfyEbGiEElyabGXHInoYhFTuTyXhupVXjYsa3CJrRGRjbLIYrHTWa9x6XBqmkdHVipcjqzVIvMn6xg7Xk83T4S6a/duajv4zLPUNj4RToQai3KM+UQjYotUIgMsGgMZJJfj76uunnAE4dzUOBqNeluD6covRKLI+YVIFDm/EIki5xciUeT8QiSKnF+IROlorT6DwSwsXxSL/HPI8kS5aHBFo1iM5A6IBapFJJkyk/QifUqRFTZ0UVtMmmvEdFEiRcXkyA2DLNISqEXm4ZGoPiZVZhmXUqenuSx69tw5atu1i8uAk9PhKLeZ2XAtwSb8DVKPyoARCTZyzti5yZEalU1b+D03OjdJ+7zlGG0/Uwjxc4WcX4hEkfMLkShyfiESRc4vRKLI+YVIlI5KfQ6De1jW8CxSS45EYMUCpWKRb1EZsMAlMSMD5mITiRwvH5FyipEEk7UaT9JIE3VGphirJ5g3vlb1BpcBmbJYjLzm7v511LbtHbxWX6w+3SyprxiTMGPvHcvz+ceiAWPHzJPFiiddDUdHXr50gfaZj678QiSKnF+IRJHzC5Eocn4hEkXOL0SiLLjbb2ZdAJ4EUG49/x/c/fNmthvAtwAMAHgWwKfdI7Wp0NxVrs6FdzDZTjoAsA3W2M5xdHc1lt8vsjvvJOAjiwSCWKS8Uy6yk17s5jbP893+cmQ3mrO4fHb1WEmxavitkEWCX2LHm6nGgoj4rvhcPbxWsfcbWCAZAI+MFQveKZW4WhHLN8noITn8YsFAb3luG8+pAPiQu9+MZjnue83sdgB/DOAr7r4XwBiAz7Q9qhBi1VnQ+b3JlfSuxdY/B/AhAP/Qan8UwMdWZIZCiBWhre8IZpZvVegdBfA4gNcAjLv7le9ppwBsW5kpCiFWgrac390b7n4LgO0AbgPwi6Gnhfqa2T4zO2hmB9nvQCFE57mm3SF3HwfwfwDcDmCdmV3ZqdgO4Azps9/dh919uBjZ9BBCdJYFnd/MNprZutbjbgAfBnAEwD8B+Letpz0A4PsrNUkhxPLTjsYwBOBRaybfywH4jrv/NzN7GcC3zOyPADwH4OF2BnRa04jLK6z0E4zLLuVymdrigTHcViyF5beYrFgAl+wakeCSeizPYCyAhMiOLOcbEJe9LBZ8VI4ELRXD3/JiY8Uku9ga14icBwC5LLzGWWSsesSWj9TkyiJSZeycLaZkHpf02i8LtqDzu/shAO8LtL+O5u9/IcTbEN3hJ0SiyPmFSBQ5vxCJIucXIlHk/EIkii1GZlj0YGbnARxv/TkIoP2EYyuH5vFmNI8383abx05339jOATvq/G8a2Oyguw+vyuCah+aheehrvxCpIucXIlFW0/n3r+LYV6N5vBnN48383M5j1X7zCyFWF33tFyJRVsX5zexeM/upmR01swdXYw6teRwzsxfN7HkzO9jBcR8xs1EzO3xV24CZPW5mr7b+X79K8/iCmZ1urcnzZvaRDsxjh5n9k5kdMbOXzOx3W+0dXZPIPDq6JmbWZWY/MbMXWvP4j6323Wb2VGs9vm1mS0uQ4e4d/Qcgj2YasD0ASgBeAHBTp+fRmssxAIOrMO6dAG4FcPiqtj8B8GDr8YMA/niV5vEFAP+uw+sxBODW1uN+AD8DcFOn1yQyj46uCZpxuX2tx0UAT6GZQOc7AD7Zav8LAL+9lHFW48p/G4Cj7v66N1N9fwvAfaswj1XD3Z8EcGle831oJkIFOpQQlcyj47j7iLs/23o8iWaymG3o8JpE5tFRvMmKJ81dDeffBuDkVX+vZvJPB/BDM3vGzPat0hyusNndR4DmmxDAplWcy2fN7FDrZ8GK//y4GjPbhWb+iKewimsybx5Ah9ekE0lzV8P5Q6lGVktyuMPdbwXwGwB+x8zuXKV5XE98DcANaNZoGAHwpU4NbGZ9AL4L4HPuPtGpcduYR8fXxJeQNLddVsP5TwHYcdXfNPnnSuPuZ1r/jwJ4DKubmeicmQ0BQOv/0dWYhLufa73xMgBfR4fWxMyKaDrcN9z9e63mjq9JaB6rtSatsa85aW67rIbzPw1gb2vnsgTgkwAOdHoSZtZrZv1XHgP4dQCH471WlANoJkIFVjEh6hVna3E/OrAm1kzs9zCAI+7+5atMHV0TNo9Or0nHkuZ2agdz3m7mR9DcSX0NwB+s0hz2oKk0vADgpU7OA8A30fz6WEPzm9BnAGwA8ASAV1v/D6zSPP4WwIsADqHpfEMdmMcH0PwKewjA861/H+n0mkTm0dE1AfBeNJPiHkLzg+Y/XPWe/QmAowD+HkB5KePoDj8hEkV3+AmRKHJ+IRJFzi9Eosj5hUgUOb8QiSLnFyJR5PxCJIqcX4hE+X8F78NzaabVgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_img= X_data[4]\n",
    "sample_label = y_data[4]\n",
    "\n",
    "print(\"Our shape of random image : \",sample_img.shape)\n",
    "\n",
    "sample_img = sample_img.reshape(32, 32, 3) #Reshape our sample image\n",
    "print(\"Our new shape of images : \", sample_img.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt #Import oyr matplotlib package\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sample_img) #Visualize the image\n",
    "print()\n",
    "print(\"Our label is : \", sample_label)\n",
    "print(\"Our image is :\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our images have a pixel values ranging from 0 - 255. So we would normalize it, so it has mean = 0 and std deviation <= 1. <br>\n",
    "we would also like to one hot encode our labels. instead of having values 0 - 9, it will have 10 columns with represent each objects. <br>\n",
    "After that, we split our data, 70% for training data, 15% for validation data, and 15% for testing data. Since the data has been spread randomly, we do not need to randomise our splitting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = preprocessing.normalize(X_data) #Normalize our images\n",
    "\n",
    "y_data = pd.get_dummies(y_data) #One hot encode our labels\n",
    "\n",
    "img_size = 32\n",
    "num_channel = 3 #RGB\n",
    "X_data = X_data.reshape(len(dataset), img_size, img_size, num_channel) #WxH = 32x32, RGB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,y_data, test_size = 0.3, random_state = 0) #split train data for 0.7\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size = 0.5, random_state = 0) #split test data for 0.15 and val data for 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the CNN Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the CNN model we will be using Tensorflow. Our CNN architecture as shown below : <br>\n",
    "![CNN Architecture](https://drive.google.com/open?id=1kWWgqA_5q9pXdaGmSudygeMTJtsT9NRo) <br>\n",
    "This architecture use the relu activation function for the hidden layers and use softmax in the output layer. Since we want to classify based on 10 different objects, then our final layer should have 10 nodes. Each node represent 1 object. <br>\n",
    "The architecture of CNN is hyperparameter, you can modify it till you get the best model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def LeNet_5 (X):\n",
    "    # Layer 1: convolution layer 5x5, input =32x32x3, output = 32x32x64\n",
    "    conv1_w = tf.get_variable(\"W1\",initializer = tf.truncated_normal(shape = [5,5,3,64], mean = 0, stddev = 0.1))\n",
    "    conv1_b = tf.get_variable(\"b1\",initializer = tf.zeros(64))\n",
    "    conv1 = tf.nn.conv2d(X, conv1_w, strides = [1,1,1,1], padding = 'SAME') + conv1_b\n",
    "    #TODO : Activation\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    #Pooling Layer. Input = 32x32x64, Output = 16x16x64\n",
    "    pool_1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    \n",
    "    #Layer 2: Convolution layer 5x5, input 16x16x64, Output = 16x16x128\n",
    "    conv2_w = tf.get_variable(\"W2\",initializer = tf.truncated_normal(shape = [5,5,64,128], mean = 0, stddev = 0.1))\n",
    "    conv2_b = tf.get_variable(\"b2\",initializer = tf.zeros(128))\n",
    "    conv2 = tf.nn.conv2d(pool_1, conv2_w, strides = [1,1,1,1], padding = 'SAME') + conv2_b\n",
    "    #TODO: Activation\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    #Pooling Layer, Input = 16x16x64, Output = 8x8x64\n",
    "    pool_2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    \n",
    "    #Layer 3: Convolution layer 5x5, input 8x8x64, Output = 8x8x256\n",
    "    conv3_w = tf.get_variable(\"W3\",initializer = tf.truncated_normal(shape = [5,5,128, 256], mean = 0, stddev = 0.1))\n",
    "    conv3_b = tf.get_variable(\"b3\",initializer = tf.zeros(256))\n",
    "    conv3 = tf.nn.conv2d(pool_2, conv3_w, strides = [1,1,1,1], padding = 'SAME') + conv3_b\n",
    "    #TODO: Activation\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    \n",
    "    #Pooling Layer, Input = 8x8x256, Output = 4x4x256\n",
    "    pool_3 = tf.nn.max_pool(conv3, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    \n",
    "    #Layer 4: Convolution layer 5x5, input 4x4x256, Output = 4x4x512\n",
    "    conv4_w = tf.get_variable(\"W4\",initializer = tf.truncated_normal(shape = [5,5,256,512], mean = 0, stddev = 0.1))\n",
    "    conv4_b = tf.get_variable(\"b4\",initializer = tf.zeros(512))\n",
    "    conv4 = tf.nn.conv2d(pool_3, conv4_w, strides = [1,1,1,1], padding = 'SAME') + conv4_b\n",
    "    #TODO: Activation\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "    \n",
    "    #Pooling Layer, Input = 4x4x512, Output = 2x2x512\n",
    "    pool_4 = tf.nn.max_pool(conv4, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    \n",
    "    #TODO : Flatten, Input = 2x2x512, Output = 2048\n",
    "    fc1 = tf.contrib.layers.flatten(pool_4)\n",
    "    \n",
    "    #Layer3: Fully Connected Layer. Input = 2048, Output = 1024\n",
    "    fc1_w = tf.get_variable(\"W5\",initializer = tf.truncated_normal(shape = (2048,1024), mean = 0, stddev = 0.08))\n",
    "    fc1_b = tf.get_variable(\"b5\",initializer = tf.zeros(1024))\n",
    "    fc1 = tf.matmul(fc1, fc1_w) + fc1_b\n",
    "    #TODO: Activation\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    #Layer4 : Fully Connected Layer. Input = 1024, Output = 512\n",
    "    fc2_w = tf.get_variable(\"W6\",initializer = tf.truncated_normal(shape = (1024, 512), mean = 0, stddev = 0.08))\n",
    "    fc2_b = tf.get_variable(\"b6\",initializer = tf.zeros(512))\n",
    "    fc2 = tf.matmul(fc1, fc2_w) + fc2_b\n",
    "    #TODO: Activation\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    #Layer5 : Fully Connected Layer. Input = 512, Output = 256\n",
    "    fc3_w = tf.get_variable(\"W7\",initializer = tf.truncated_normal(shape = (512, 256), mean = 0, stddev = 0.08))\n",
    "    fc3_b = tf.get_variable(\"b7\",initializer = tf.zeros(256))\n",
    "    fc3 = tf.matmul(fc2, fc3_w) + fc3_b\n",
    "    #TODO: Activation\n",
    "    fc3 = tf.nn.relu(fc3)\n",
    "    \n",
    "    #Layer6 : Fully Connected Layer. Input = 256, Output = 128\n",
    "    fc4_w = tf.get_variable(\"W8\", initializer = tf.truncated_normal(shape = (256, 128), mean = 0, stddev = 0.08))\n",
    "    fc4_b = tf.get_variable(\"b8\", initializer = tf.zeros(128))\n",
    "    fc4 = tf.matmul(fc3, fc4_w) + fc4_b\n",
    "    #TODO: Activation\n",
    "    fc4 = tf.nn.relu(fc4)\n",
    "    \n",
    "    #Layer7 : Fully Connected Layer. Input = 128, Output = 64\n",
    "    fc5_w = tf.get_variable(\"W9\", initializer = tf.truncated_normal(shape = (128, 64), mean = 0, stddev = 0.08))\n",
    "    fc5_b = tf.get_variable(\"b9\", initializer = tf.zeros(64))\n",
    "    fc5 = tf.matmul(fc4, fc5_w) + fc5_b\n",
    "    #TODO: Activation\n",
    "    fc5 = tf.nn.relu(fc5)\n",
    "    \n",
    "    #Layer5 : Fully Connected Layer, Input = 64, Output = 10\n",
    "    fc6_w = tf.get_variable(\"W10\", initializer = tf.truncated_normal(shape = (64, 10), mean = 0, stddev = 0.08))\n",
    "    fc6_b = tf.get_variable(\"b10\", initializer = tf.zeros(10))\n",
    "    logits = tf.matmul(fc5, fc6_w) + fc6_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the images and labels that we will assign to the model in placeholder. X for the images and y for the labels. Placeholder in tensorflow are used to feed external data into TensorFlow graph. <br> Check out [this](https://www.quora.com/What-is-the-difference-between-Variables-and-Placeholders-in-tensor-flow) Q&A discussion in quora for detail explanation about TensorFlow's placeholder.<br><br>\n",
    "we define the model in the main code as logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From G:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 32,32,3]) #X placeholder\n",
    "y = tf.placeholder(tf.int32, shape = [None,10]) #y placeholder\n",
    "\n",
    "logits = LeNet_5(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the categorical_cross_entropy as our loss function and Adam as our optimizer. We update our model's weights and bias by reducing our loss function. We use learning rate of 0.0015. The learning rate value is hyperparameter. If the learning rate value is too high then it will oscillate around the minimum value. While if the learning rate is too low, it will be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y, logits = logits) #compute the cross entropy losses\n",
    "loss_operation = tf.reduce_mean(cross_entropy) #Reduce the cross entropy losses into single unit by taking the mean of it\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.0015)\n",
    "training_operation = optimizer.minimize(loss_operation) #Backpropragate our model using Adam optimizer\n",
    "\n",
    "#To train and evaluate the model\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y,1)) #sum all the match prediction and labels\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #Store the accuracy of our input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model with 50 iterations/epochs. each iteration runs for 128 batches size. In TensorFlow, we can only run our model inside a session. Our training process will run inside the session. We are training with 128 batches size. If we have X images for our training set. Then, each process of training will take X/128 images instead of a single image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... with dataset -  35000\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy =  0.394\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy =  0.508\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy =  0.558\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy =  0.581\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy =  0.625\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy =  0.608\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy =  0.593\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy =  0.608\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy =  0.596\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy =  0.610\n",
      "\n",
      "Model saves \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128    \n",
    "    \n",
    "#Train data till 50 epochs\n",
    "with tf.Session() as sess:    \n",
    "    sess.run(tf.global_variables_initializer()) #Initialize global variables\n",
    "    num_examples = len(X_train)\n",
    "      \n",
    "    print(\"Training... with dataset - \", num_examples)\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        step = int(len(X_train)/BATCH_SIZE)\n",
    "        for offset in range(0, BATCH_SIZE):\n",
    "            batch_X, batch_y = X_train[step*offset:step*(offset+1)], y_train[step*offset:step*(offset+1)]\n",
    "            sess.run(training_operation, feed_dict = {X: batch_X, y: batch_y})\n",
    "        \n",
    "        validation_accuracy = sess.run(accuracy_operation, feed_dict = {X: X_val, y: y_val})\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy =  {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    '''Saving the trained model'''\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, 'tmp1/convnet.ckpt') \n",
    "    print(\"Model saves \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After We have succesfully trained our model, then we should test it with our test set. Test set should not be feeded in the training process. Because we want to test how accurate our model to classify unseen images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp1/convnet.ckpt\n",
      "Test Accuracy =  0.6164\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    model = saver.restore(sess, \"tmp1/convnet.ckpt\")\n",
    "    test_accuracy = sess.run(accuracy_operation, feed_dict = {X: X_test, y: y_test})\n",
    "    print(\"Test Accuracy = \", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

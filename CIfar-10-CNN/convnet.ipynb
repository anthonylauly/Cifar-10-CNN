{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewnngQzLg_bW"
   },
   "source": [
    "# Image Classification with Cifar-10 Dataset Using CNN on Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmtUXPy9g_bX"
   },
   "source": [
    "&emsp;&emsp;In this notebook, I am going to make a classifier for CIFAR-10 Dataset using Convolutional Neural Network. The classifier is CNN based model. The CNN model should be feed with the images that has been preprocessed and normalized first. The classifier should then test the test set and make a classification of each test set. This notebook was run in NVIDIA GTX 1050, and intel Core i5 with cuda 10.1, cudnn 10.1, using tensorflow 1.13.1\n",
    "\n",
    "&emsp;&emsp; The purpose of this notebook is to introduce the implementation of tensorflow for Convolutional Neural Network. This notebook will cover simple explanation about the layers, activation, loss function, and optimizer that I used. This notebook is not suitable for those who are already expert in Machine Learning/Deep Learning. This notebook is go for a great accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8CCmRBvmg_bX"
   },
   "source": [
    "# Get the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgDa25JOg_bY"
   },
   "source": [
    "&emsp;&emsp;The dataset of Cifar-10 can be found [here](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)\n",
    ". For a simplicity purpose, I have combined and cleaned the datasets so all that is left is just images and labels. To combined and clean the datasets you can use pickle package. Go check [this blogspot](https://luckydanny.blogspot.com/2016/07/load-cifar-10-dataset-in-python3.html) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ysMtV6lg_bY"
   },
   "source": [
    "&emsp;&emsp;First import the preprocessing library that will be useful for our project. Pandas package is use for data structure and data manipulation. preprocessing package from sklearn is used to normalize our images. train_test_split package is used to split our dataset as test images, train images, and validation images.<br>\n",
    "\n",
    "For the pandas user-guide, you can check out [here](http://pandas.pydata.org/pandas-docs/stable/user_guide/index.html) <br>\n",
    "For the sklearn documentation, you can check out [here](https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hkHcwPIjg_bZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Skjf4I4Eg_bb"
   },
   "source": [
    "&emsp;&emsp;Read the dataset (images and labels) then store it in dataset variable. Our images size is 32x32x3 = 3072 columns.Colored image has 3 layers as shown below :\n",
    "<img src=\"250px-RGBLayers.svg.png\" style=\"width: 200px; height: 200px;\"> <br>\n",
    "Each layer has 32x32 = 1024 pixels. Since it is RGB image, then it has 3 layers consist of Red, Green, and Blue layers.<br> \n",
    "&emsp;&emsp;In our dataset. The 1st until the 3072th columns is the pixel, while the last column is our labels. We store the images (pixel) in X_data variable and the labels in y_data variable. <br>\n",
    "Lets print out the first five rows of our images and labels. Since the index in python starts from 0, so to show first 5 rows ,then we should print out the index of 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "4byTj4Rfg_bc",
    "outputId": "72d9cd7d-4de6-4f19-cf3a-6ac1e6518a90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our images samples : \n",
      "   0     1     2     3     4     5     ...  3066  3067  3068  3069  3070  3071\n",
      "0    59    62    63    43    46    45  ...   151   118    84   123    92    72\n",
      "1   154   177   187   126   137   136  ...   143   134   142   143   133   144\n",
      "2   255   255   255   253   253   253  ...    79    85    83    80    86    84\n",
      "3    28    25    10    37    34    19  ...    63    56    37    72    65    46\n",
      "4   170   180   198   168   178   196  ...    71    75    78    73    77    80\n",
      "\n",
      "[5 rows x 3072 columns]\n",
      "Our labels samples : \n",
      "   0\n",
      "0  6\n",
      "1  9\n",
      "2  9\n",
      "3  4\n",
      "4  1\n"
     ]
    }
   ],
   "source": [
    "#dataset = pd.read_csv(\"dataset.csv\")\n",
    "X_data = dataset.iloc[:, :-1].values\n",
    "print(\"Our images samples : \")\n",
    "print(pd.DataFrame(X_data).head(5))\n",
    "\n",
    "y_data = dataset.iloc[:, -1].values\n",
    "print(\"Our labels samples : \")\n",
    "print(pd.DataFrame(y_data).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXJPJwETg_bg"
   },
   "source": [
    "Our labels is an integer from 0 - 9 which is stand for : <br>\n",
    "- airplane   : 0 <br>\n",
    "- automobile : 1 <br>\n",
    "- bird       : 2 <br>\n",
    "- cat        : 3 <br>\n",
    "- deer       : 4 <br>\n",
    "- dog        : 5 <br>\n",
    "- frog       : 6 <br>\n",
    "- horse      : 7 <br>\n",
    "- ship       : 8 <br>\n",
    "- truck      : 9\n",
    "\n",
    "&emsp;&emsp; By translating the number labels from our label's dictionary, we can definitely say that our first image is frog, the second image is truck, the third image is truck, the fourth image is deer, and so on. To prove that, we can also visualize the image. But you should notice that our image is just 32 x 32 pixel so it will be blurry for our sight. We take the fifth image from our dataset as our sample image. To visualize the image, we should first reshape our sample image from the shape of (_,3072) into (_,32,32,3). Then we visualize the image using matplotlib package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "2TAeKxSPg_bh",
    "outputId": "ea3fc78b-5e6e-45ab-d5eb-043d656508e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our shape of random image :  (3072,)\n",
      "Our new shape of images :  (32, 32, 3)\n",
      "\n",
      "Our label is :  1\n",
      "Our image is :\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIlHk/EIkyv8Dbs3Lfjigaw4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_img= X_data[4]\n",
    "sample_label = y_data[4]\n",
    "\n",
    "print(\"Our shape of random image : \",sample_img.shape)\n",
    "\n",
    "sample_img = sample_img.reshape(32, 32, 3) #Reshape our sample image\n",
    "print(\"Our new shape of images : \", sample_img.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt #Import oyr matplotlib package\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sample_img) #Visualize the image\n",
    "print()\n",
    "print(\"Our label is : \", sample_label)\n",
    "print(\"Our image is :\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3u3TbYlzg_bj"
   },
   "source": [
    "# Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7fDEAirg_bj"
   },
   "source": [
    "&emsp;&emsp;The first preprocess step is to normalize our data (images). In our project, this step is an optional step, since all the features(pixel) is in the same range. Each pixel has value range 0 - 255. We normalize the data using preprocessing library from sklearn. To check know more about the importance of normalization, you can check out this [medium article](https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029) <br>\n",
    "&emsp;&emsp;For the labels, we would like to one hot encode it. One hot encoding is a process to change categorical variables into a form that could provide better result for the CNN model. Instead of having values 0 - 9 for the labels, it will have 10 columns, and each column represents each objects. To understand it more about one hot encoder and how it works, lets print out the labels before and after one hot encoded. <br>\n",
    "&emsp;&emsp;After that, we split our data, 70% for training data, 15% for validation data, and 15% for testing data. Since the data has been spread randomly, we do not need to randomise our splitting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "colab_type": "code",
    "id": "XDcW_vQzg_bk",
    "outputId": "ddfab0e7-c6e9-46cd-c42e-9f663e18591d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image-labels frequency in Training Data\n",
      "6    3545\n",
      "8    3519\n",
      "4    3517\n",
      "3    3509\n",
      "5    3501\n",
      "7    3486\n",
      "2    3486\n",
      "0    3486\n",
      "9    3477\n",
      "1    3474\n",
      "dtype: int64\n",
      "\n",
      "Image-labels frequency in Testing Data\n",
      "9    780\n",
      "1    764\n",
      "0    761\n",
      "2    750\n",
      "6    748\n",
      "5    747\n",
      "7    741\n",
      "4    740\n",
      "3    738\n",
      "8    731\n",
      "dtype: int64\n",
      "\n",
      "Image-labels frequency in Validation Data\n",
      "7    773\n",
      "2    764\n",
      "1    762\n",
      "3    753\n",
      "0    753\n",
      "5    752\n",
      "8    750\n",
      "9    743\n",
      "4    743\n",
      "6    707\n",
      "dtype: int64\n",
      "\n",
      "Labels before One-hot Encoded\n",
      "   0\n",
      "0  4\n",
      "1  4\n",
      "2  2\n",
      "3  2\n",
      "4  1\n",
      "\n",
      "Labels after One-hot Encoded\n",
      "   0  1  2  3  4  5  6  7  8  9\n",
      "0  0  0  0  0  1  0  0  0  0  0\n",
      "1  0  0  0  0  1  0  0  0  0  0\n",
      "2  0  0  1  0  0  0  0  0  0  0\n",
      "3  0  0  1  0  0  0  0  0  0  0\n",
      "4  0  1  0  0  0  0  0  0  0  0\n"
     ]
    }
   ],
   "source": [
    "X_data = preprocessing.normalize(X_data) #Normalize our images (Optional  process)\n",
    "\n",
    "img_size = 32\n",
    "num_channel = 3 #RGB\n",
    "X_data = X_data.reshape(len(dataset), img_size, img_size, num_channel) #WxH = 32x32, RGB\n",
    "\n",
    "'''Data Splitting Process'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,y_data, test_size = 0.3, random_state = 0) #split train data for 0.7\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size = 0.5, random_state = 0) #split test data for 0.15 and val data for 0.15\n",
    "\n",
    "print(\"Image-labels frequency in Training Data\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print()\n",
    "\n",
    "print(\"Image-labels frequency in Testing Data\")\n",
    "print(pd.Series(y_test).value_counts())\n",
    "print()\n",
    "\n",
    "print(\"Image-labels frequency in Validation Data\")\n",
    "print(pd.Series(y_val).value_counts())\n",
    "print()\n",
    "\n",
    "'''One Hot Encoding Process'''\n",
    "print (\"Labels before One-hot Encoded\")\n",
    "print(pd.DataFrame(y_train).head(5))\n",
    "print()\n",
    "\n",
    "y_train = pd.get_dummies(y_train) #One hot encode our labels\n",
    "y_test = pd.get_dummies(y_test) #One hot encode our labels\n",
    "y_val = pd.get_dummies(y_val) #One hot encode our labels\n",
    "print(\"Labels after One-hot Encoded\")\n",
    "print(pd.DataFrame(y_train).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WkQH5Qdg_bn"
   },
   "source": [
    "# Create the CNN Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4ke--ykg_bn"
   },
   "source": [
    "To create the CNN model we will be using Tensorflow. Our CNN architecture as shown below : <br>\n",
    "<img src=\"convnets.png\">  <br>\n",
    "&emsp;&emsp;This architecture use the leaky relu activation function for the hidden layers and softmax activation function in the output layer. Since we want to classify based on 10 different objects, then our final layer should have 10 nodes. Each node represent 1 object. In this example, the CNN architecture is built on 13 layers (1 input layer + 11 hidden layers + 1 output layer). Hidden layers consist of Convolution layer, max pooling layer, and Fully-Connected Layer. <br>\n",
    "- Convolution Layer : <br>\n",
    "&emsp;&emsp;Convolution Layer works differently from the ordinary fully connected (FC) layer. FC layer requires the input matrix/array to be in 1-dimension. If we have RGB image with size of 28 x 28, means that we have 3 matrixes with each matrix's size is 28x28. For FC layer we should flatten all those matrixes so that we have a single row of matrix with size of 28x28x3 = 2352. Whereas in convolution layer, we doesnt need to flatten our matrix, because the work of convolution layer (Conv2D) is by taking 2 dimensions of matrix directly. Here is a detail work of Convolution 2D layer works :\n",
    "<img src=\"conv_input.png\">\n",
    "<img src=\"conv_layer.gif\">\n",
    "\n",
    "&emsp;&emsp;The reason why we use convolution layer is because conv layer is more likely to be able to capture the spatial and temporal dependencies in an image \n",
    "&emsp;&emsp;through the application of relevant filters. For more explanation about convolution layer, you can check out this [article](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) \n",
    "<br><br>\n",
    "\n",
    "- Pooling Layer : <br>\n",
    "Pooling layer is designed to reduce the spatial size of the convolved features. Frankly speaking, you can also use Convolution layer to reduce its spatial size by increasing the stride size. But people tend to use max-pooling layer to reduce the spatial size. Max pooling works by reducing kxk size into a single matrix by taking the maximum value in the kxk matrix. For easier explanation, you can see this animation, taken from [here](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) article.\n",
    "<img src=\"pooled_layer.gif\">\n",
    "In this illustration. The input is 5x5 matrix is max pooled by 3x3 ksize, yielding 3x3 matrix.<br><br>\n",
    "\n",
    "- Fully Connected Layer : <br>\n",
    "\"The fully connected (FC) layer in the CNN represents the feature vector for the input. The convolution layers before the FC layer(s) hold information regarding local features in the input image such as edges, blobs, shapes, etc. Each conv layer hold several filters that represent one of the local features. The FC layer holds composite and aggregated information from all the conv layers that matters the most.\" Cited from [here](https://www.quora.com/What-happens-in-the-fully-connected-layer-in-a-convolutional-neural-network)\n",
    "<img src=\"fully_connected.png\"> <br>\n",
    "\n",
    "- Leaky ReLU Activation : <br>\n",
    "<img src=\"leaky_relu.png\">\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;Unlike ReLU that eliminate the negative part of the function, Leaky ReLU does not completely vanish it. Leaky ReLU just reduce the magnitude of it \n",
    "&emsp;&emsp;by multiply it with 0.01 . Leaky ReLU tackles the vanishing gradient problem, and also there is no longer saturation region. For more information about \n",
    "&emsp;&emsp;vanishing gradient, you can check this [article](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b) <br>\n",
    "\n",
    "&emsp;&emsp;The architecture of CNN is hyperparameter, you can modify it till you get the best model's accuracy. To check the best Convnets architecture, you can check out the collection of the research's paper [here](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def Convnet (X):\n",
    "    conv_W1 = tf.get_variable('conv_W1', initializer = tf.truncated_normal(shape = [3,3,3,32], mean = 0, stddev = 0.1))\n",
    "    conv_b1 = tf.get_variable('conv_b1', initializer = tf.truncated_normal(shape = [32]))\n",
    "    conv1 = tf.nn.conv2d(X,conv_W1, strides = [1,1,1,1], padding = 'SAME') + conv_b1\n",
    "    conv1 = tf.nn.leaky_relu(conv1)\n",
    "\n",
    "    conv_W2 = tf.get_variable('conv_W2', initializer = tf.truncated_normal(shape = [3,3,32,48], mean = 0, stddev = 0.1))\n",
    "    conv_b2 = tf.get_variable('conv_b2', initializer = tf.truncated_normal(shape = [48]))\n",
    "    conv2 = tf.nn.conv2d(conv1, conv_W2, strides = [1,1,1,1], padding = 'SAME') + conv_b2\n",
    "    conv2 = tf.nn.leaky_relu(conv2)\n",
    "\n",
    "    max_pool1 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "    conv_W3 = tf.get_variable('conv_W3', initializer = tf.truncated_normal(shape = [3,3,48,80], mean = 0, stddev = 0.1))\n",
    "    conv_b3 = tf.get_variable('conv_b3', initializer = tf.truncated_normal(shape = [80]))\n",
    "    conv3 = tf.nn.conv2d(max_pool1, conv_W3, strides = [1,1,1,1], padding = 'SAME') + conv_b3\n",
    "    conv3 = tf.nn.leaky_relu(conv3)\n",
    "\n",
    "    conv_W4 = tf.get_variable('conv_W4', initializer = tf.truncated_normal(shape = [3,3,80,80], mean = 0, stddev = 0.1))\n",
    "    conv_b4 = tf.get_variable('conv_b4', initializer = tf.truncated_normal(shape = [80]))\n",
    "    conv4 = tf.nn.conv2d(conv3, conv_W4, strides = [1,1,1,1], padding = 'SAME') + conv_b4\n",
    "    conv4 = tf.nn.leaky_relu(conv4)\n",
    "\n",
    "    max_pool2 = tf.nn.max_pool(conv4, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "    conv_W5 = tf.get_variable('conv_W5', initializer = tf.truncated_normal(shape = [3,3,80,128], mean = 0, stddev = 0.1))\n",
    "    conv_b5 = tf.get_variable('conv_b5', initializer = tf.truncated_normal(shape = [128]))\n",
    "    conv5 = tf.nn.conv2d(max_pool2, conv_W5, strides = [1,1,1,1], padding = 'SAME') + conv_b5\n",
    "    conv5 = tf.nn.leaky_relu(conv5)\n",
    "    \n",
    "    conv_W6 = tf.get_variable('conv_W6', initializer = tf.truncated_normal(shape = [3,3,128,128], mean = 0, stddev = 0.1))\n",
    "    conv_b6 = tf.get_variable('conv_b6', initializer = tf.truncated_normal(shape = [128]))\n",
    "    conv6 = tf.nn.conv2d(conv5, conv_W6, strides = [1,1,1,1], padding = 'SAME') + conv_b6\n",
    "    conv6 = tf.nn.leaky_relu(conv6)\n",
    "    \n",
    "    max_pool3 = tf.nn.max_pool(conv6, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    \n",
    "    fc1 = tf.contrib.layers.flatten(max_pool3)\n",
    "\n",
    "    W1 = tf.get_variable('W1', initializer = tf.truncated_normal(shape = [2048,500], mean = 0, stddev = 0.1))\n",
    "    b1 = tf.get_variable('b1', initializer = tf.truncated_normal(shape = [500]))\n",
    "    fc1 = tf.matmul (fc1, W1) + b1\n",
    "    fc1 = tf.nn.leaky_relu(fc1)\n",
    "\n",
    "    W2 = tf.get_variable('W2', initializer = tf.truncated_normal(shape = [500,10], mean = 0, stddev = 0.1))\n",
    "    b2 = tf.get_variable('b2', initializer = tf.truncated_normal(shape = [10]))\n",
    "    logit = tf.matmul(fc1, W2) + b2\n",
    "    \n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YgshmUrg_bp"
   },
   "source": [
    "&emsp;&emsp;We store the images and labels that we will assign to the model in placeholder. X for the images and y for the labels. Placeholder in tensorflow are used to feed external data into TensorFlow graph. <br> Check out [this](https://www.quora.com/What-is-the-difference-between-Variables-and-Placeholders-in-tensor-flow) Q&A discussion in quora for detail explanation about TensorFlow's placeholder.<br><br>\n",
    "logits is the final node in our model before it feeds to the final activation function (softmax activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "KzgLRigUg_bq",
    "outputId": "4f6d35ec-5996-4116-8367-dd3a17624cc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0615 14:44:22.509925 140653830354816 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0615 14:44:22.512106 140653830354816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #To make sure that we do not overlap any variables in the graph\n",
    "X = tf.placeholder(tf.float32, shape = [None, 32,32,3]) #X placeholder\n",
    "y = tf.placeholder(tf.int32, shape = [None,10]) #y placeholder\n",
    "\n",
    "logits = Convnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3ln2Td9g_bs"
   },
   "source": [
    "We will be using the categorical_cross_entropy as our loss function and Adam as our optimizer. We update our model's weights and bias by reducing our loss function.\n",
    "- Categorical Cross Entropy : <br>\n",
    "$$L = - \\sum\\limits_{i} y_i \\log {P_i}$$\n",
    "$$ y_i = Ground Truth $$\n",
    "$$P_i = model's Prediction$$\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;Cross entropy computes the distance between the distribution that the model's predict and with the ground truth distribution. It is widely used when \n",
    "&emsp;&emsp;node activations can be understood as representing the probability that each hypothesis might be true, i.e. when the output is a probability distribution. \n",
    "&emsp;&emsp;Thus it is used as a loss function in neural networks which have softmax activations in the output layer. To find out more about the cross entropy loss and \n",
    "&emsp;&emsp;its derivative with softmax activation function, check this [article](https://gombru.github.io/2018/05/23/cross_entropy_loss/)\n",
    "\n",
    "- Adam Optimizer : <br>\n",
    "&emsp;&emsp;Adaptive Moment Estimation (Adam) is a method to compute adaptive learning rates for each parameter. To find out about optimizer and its variance then check out this [article](http://ruder.io/optimizing-gradient-descent/index.html#adam)\n",
    "\n",
    "In this notebook, we use learning rate of 0.001. The learning rate value is hyperparameter. If the learning rate value is too high then it will oscillate around the minimum value. While if the learning rate is too low, it will be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyKh3IBpg_bs"
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y, logits = logits) #compute the cross entropy losses\n",
    "loss_operation = tf.reduce_mean(cross_entropy) #Reduce the cross entropy losses into single unit by taking the mean of it\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "training_operation = optimizer.minimize(loss_operation) #Backpropragate our model using Adam optimizer\n",
    "\n",
    "#To train and evaluate the model\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y,1)) #sum all the match prediction and labels\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #Store the accuracy of our input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5SM__Gjg_bu"
   },
   "source": [
    "# Train our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ph-CPbWzg_bu"
   },
   "source": [
    "&emsp;&emsp;We train our model with 100 iterations/epochs. each iteration runs for 128 batches size. 128 batches size means that for X images in our training set, each process of training will take X/128 images instead of a single image. In TensorFlow, we can only run our model inside a session. Our training process will run inside the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5107
    },
    "colab_type": "code",
    "id": "mwA0i68Tg_bv",
    "outputId": "48cb1db8-eeaa-437f-ed72-1fcde6c767b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... with dataset -  35000\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy =  0.111\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy =  0.103\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy =  0.144\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy =  0.130\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy =  0.151\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy =  0.168\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy =  0.243\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy =  0.281\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy =  0.313\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy =  0.360\n",
      "\n",
      "EPOCH 11 ...\n",
      "Validation Accuracy =  0.372\n",
      "\n",
      "EPOCH 12 ...\n",
      "Validation Accuracy =  0.382\n",
      "\n",
      "EPOCH 13 ...\n",
      "Validation Accuracy =  0.397\n",
      "\n",
      "EPOCH 14 ...\n",
      "Validation Accuracy =  0.401\n",
      "\n",
      "EPOCH 15 ...\n",
      "Validation Accuracy =  0.410\n",
      "\n",
      "EPOCH 16 ...\n",
      "Validation Accuracy =  0.431\n",
      "\n",
      "EPOCH 17 ...\n",
      "Validation Accuracy =  0.429\n",
      "\n",
      "EPOCH 18 ...\n",
      "Validation Accuracy =  0.438\n",
      "\n",
      "EPOCH 19 ...\n",
      "Validation Accuracy =  0.433\n",
      "\n",
      "EPOCH 20 ...\n",
      "Validation Accuracy =  0.445\n",
      "\n",
      "EPOCH 21 ...\n",
      "Validation Accuracy =  0.461\n",
      "\n",
      "EPOCH 22 ...\n",
      "Validation Accuracy =  0.456\n",
      "\n",
      "EPOCH 23 ...\n",
      "Validation Accuracy =  0.455\n",
      "\n",
      "EPOCH 24 ...\n",
      "Validation Accuracy =  0.444\n",
      "\n",
      "EPOCH 25 ...\n",
      "Validation Accuracy =  0.457\n",
      "\n",
      "EPOCH 26 ...\n",
      "Validation Accuracy =  0.460\n",
      "\n",
      "EPOCH 27 ...\n",
      "Validation Accuracy =  0.473\n",
      "\n",
      "EPOCH 28 ...\n",
      "Validation Accuracy =  0.477\n",
      "\n",
      "EPOCH 29 ...\n",
      "Validation Accuracy =  0.469\n",
      "\n",
      "EPOCH 30 ...\n",
      "Validation Accuracy =  0.486\n",
      "\n",
      "EPOCH 31 ...\n",
      "Validation Accuracy =  0.497\n",
      "\n",
      "EPOCH 32 ...\n",
      "Validation Accuracy =  0.500\n",
      "\n",
      "EPOCH 33 ...\n",
      "Validation Accuracy =  0.511\n",
      "\n",
      "EPOCH 34 ...\n",
      "Validation Accuracy =  0.526\n",
      "\n",
      "EPOCH 35 ...\n",
      "Validation Accuracy =  0.520\n",
      "\n",
      "EPOCH 36 ...\n",
      "Validation Accuracy =  0.512\n",
      "\n",
      "EPOCH 37 ...\n",
      "Validation Accuracy =  0.511\n",
      "\n",
      "EPOCH 38 ...\n",
      "Validation Accuracy =  0.526\n",
      "\n",
      "EPOCH 39 ...\n",
      "Validation Accuracy =  0.507\n",
      "\n",
      "EPOCH 40 ...\n",
      "Validation Accuracy =  0.499\n",
      "\n",
      "EPOCH 41 ...\n",
      "Validation Accuracy =  0.499\n",
      "\n",
      "EPOCH 42 ...\n",
      "Validation Accuracy =  0.514\n",
      "\n",
      "EPOCH 43 ...\n",
      "Validation Accuracy =  0.524\n",
      "\n",
      "EPOCH 44 ...\n",
      "Validation Accuracy =  0.512\n",
      "\n",
      "EPOCH 45 ...\n",
      "Validation Accuracy =  0.501\n",
      "\n",
      "EPOCH 46 ...\n",
      "Validation Accuracy =  0.513\n",
      "\n",
      "EPOCH 47 ...\n",
      "Validation Accuracy =  0.511\n",
      "\n",
      "EPOCH 48 ...\n",
      "Validation Accuracy =  0.522\n",
      "\n",
      "EPOCH 49 ...\n",
      "Validation Accuracy =  0.525\n",
      "\n",
      "EPOCH 50 ...\n",
      "Validation Accuracy =  0.524\n",
      "\n",
      "EPOCH 51 ...\n",
      "Validation Accuracy =  0.508\n",
      "\n",
      "EPOCH 52 ...\n",
      "Validation Accuracy =  0.522\n",
      "\n",
      "EPOCH 53 ...\n",
      "Validation Accuracy =  0.541\n",
      "\n",
      "EPOCH 54 ...\n",
      "Validation Accuracy =  0.523\n",
      "\n",
      "EPOCH 55 ...\n",
      "Validation Accuracy =  0.519\n",
      "\n",
      "EPOCH 56 ...\n",
      "Validation Accuracy =  0.529\n",
      "\n",
      "EPOCH 57 ...\n",
      "Validation Accuracy =  0.535\n",
      "\n",
      "EPOCH 58 ...\n",
      "Validation Accuracy =  0.523\n",
      "\n",
      "EPOCH 59 ...\n",
      "Validation Accuracy =  0.538\n",
      "\n",
      "EPOCH 60 ...\n",
      "Validation Accuracy =  0.530\n",
      "\n",
      "EPOCH 61 ...\n",
      "Validation Accuracy =  0.542\n",
      "\n",
      "EPOCH 62 ...\n",
      "Validation Accuracy =  0.539\n",
      "\n",
      "EPOCH 63 ...\n",
      "Validation Accuracy =  0.542\n",
      "\n",
      "EPOCH 64 ...\n",
      "Validation Accuracy =  0.540\n",
      "\n",
      "EPOCH 65 ...\n",
      "Validation Accuracy =  0.534\n",
      "\n",
      "EPOCH 66 ...\n",
      "Validation Accuracy =  0.541\n",
      "\n",
      "EPOCH 67 ...\n",
      "Validation Accuracy =  0.541\n",
      "\n",
      "EPOCH 68 ...\n",
      "Validation Accuracy =  0.540\n",
      "\n",
      "EPOCH 69 ...\n",
      "Validation Accuracy =  0.542\n",
      "\n",
      "EPOCH 70 ...\n",
      "Validation Accuracy =  0.536\n",
      "\n",
      "EPOCH 71 ...\n",
      "Validation Accuracy =  0.542\n",
      "\n",
      "EPOCH 72 ...\n",
      "Validation Accuracy =  0.542\n",
      "\n",
      "EPOCH 73 ...\n",
      "Validation Accuracy =  0.546\n",
      "\n",
      "EPOCH 74 ...\n",
      "Validation Accuracy =  0.549\n",
      "\n",
      "EPOCH 75 ...\n",
      "Validation Accuracy =  0.551\n",
      "\n",
      "EPOCH 76 ...\n",
      "Validation Accuracy =  0.557\n",
      "\n",
      "EPOCH 77 ...\n",
      "Validation Accuracy =  0.537\n",
      "\n",
      "EPOCH 78 ...\n",
      "Validation Accuracy =  0.556\n",
      "\n",
      "EPOCH 79 ...\n",
      "Validation Accuracy =  0.552\n",
      "\n",
      "EPOCH 80 ...\n",
      "Validation Accuracy =  0.557\n",
      "\n",
      "EPOCH 81 ...\n",
      "Validation Accuracy =  0.548\n",
      "\n",
      "EPOCH 82 ...\n",
      "Validation Accuracy =  0.556\n",
      "\n",
      "EPOCH 83 ...\n",
      "Validation Accuracy =  0.550\n",
      "\n",
      "EPOCH 84 ...\n",
      "Validation Accuracy =  0.550\n",
      "\n",
      "EPOCH 85 ...\n",
      "Validation Accuracy =  0.557\n",
      "\n",
      "EPOCH 86 ...\n",
      "Validation Accuracy =  0.559\n",
      "\n",
      "EPOCH 87 ...\n",
      "Validation Accuracy =  0.556\n",
      "\n",
      "EPOCH 88 ...\n",
      "Validation Accuracy =  0.550\n",
      "\n",
      "EPOCH 89 ...\n",
      "Validation Accuracy =  0.558\n",
      "\n",
      "EPOCH 90 ...\n",
      "Validation Accuracy =  0.570\n",
      "\n",
      "EPOCH 91 ...\n",
      "Validation Accuracy =  0.567\n",
      "\n",
      "EPOCH 92 ...\n",
      "Validation Accuracy =  0.563\n",
      "\n",
      "EPOCH 93 ...\n",
      "Validation Accuracy =  0.557\n",
      "\n",
      "EPOCH 94 ...\n",
      "Validation Accuracy =  0.568\n",
      "\n",
      "EPOCH 95 ...\n",
      "Validation Accuracy =  0.570\n",
      "\n",
      "EPOCH 96 ...\n",
      "Validation Accuracy =  0.570\n",
      "\n",
      "EPOCH 97 ...\n",
      "Validation Accuracy =  0.554\n",
      "\n",
      "EPOCH 98 ...\n",
      "Validation Accuracy =  0.571\n",
      "\n",
      "EPOCH 99 ...\n",
      "Validation Accuracy =  0.571\n",
      "\n",
      "EPOCH 100 ...\n",
      "Validation Accuracy =  0.563\n",
      "\n",
      "Model saves \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128    \n",
    "    \n",
    "#Train data till 100 epochs\n",
    "with tf.Session() as sess:    \n",
    "    sess.run(tf.global_variables_initializer()) #Initialize global variables\n",
    "    num_examples = len(X_train)\n",
    "      \n",
    "    print(\"Training... with dataset - \", num_examples)\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        step = int(len(X_train)/BATCH_SIZE)\n",
    "        for offset in range(0, BATCH_SIZE):\n",
    "            batch_X, batch_y = X_train[step*offset:step*(offset+1)], y_train[step*offset:step*(offset+1)]\n",
    "            sess.run(training_operation, feed_dict = {X: batch_X, y: batch_y})\n",
    "        \n",
    "        validation_accuracy = sess.run(accuracy_operation, feed_dict = {X: X_val, y: y_val})\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy =  {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    '''Saving the trained model'''\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, 'tmp1/convnet.ckpt') \n",
    "    print(\"Model saves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;In our final epoch, we will see that our validation accuracy only reach 56,3% . This is not a good model to be deployed. This lack of accuraccy caused by many factors. One of the factors could be that the architecture is not good enough because the Convolution layers could not able to catch the features of the images. Another possible reason is because we trained our model for only a few epochs. To reach a better accuracy or even to reach the state of art, you should do more reserach on the CNN architecture or read a lot of papers about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kq3vP3htg_bx"
   },
   "source": [
    "# Test our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdfXin8xg_by"
   },
   "source": [
    "&emsp;&emsp;After We have succesfully trained our model, then we should test it with our test set. Test set should not be feeded in the training process. For a benchmark, we do not really care about the training loss or accuracy. The most important is the loss or accuracy of the unseen images. It is the reason why test set should not be feeded in the training process. Test set could represent our real life images that the model has not seen in training process. Test set loss and accuracy could be the reference to tell wether our model is overfit or underfit to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "VXoG7Wh6g_bz",
    "outputId": "dfd3e2e1-d5e2-431f-b855-caea9dfb8af1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0615 16:06:13.722228 140653830354816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.5646667\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    model = saver.restore(sess, \"tmp1/convnet.ckpt\")\n",
    "    test_accuracy = sess.run(accuracy_operation, feed_dict = {X: X_test, y: y_test})\n",
    "    print(\"Test Accuracy = \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;For the test set, we got the accuracy of our model in 56,47% which cannot be considered as a good model to be used. For a reminder that, this notebook is not created to reach the state of art performance for CIFAR-10. This notebook is created to guide us for the implementation of tensorflow for CNN. To create a great model that will reach a good accuracy will be covered in the next notebook."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lenet5_cifar_10.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
